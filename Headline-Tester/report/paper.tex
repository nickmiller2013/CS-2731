%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Using Surface and Lexical Features for Headline--Body Stance Detection}

\author{Nathaniel Blake, Nick Miller, and Matthew O'Brien \\
  University of Pittsburgh \\
	{\tt \{nsb23, nam99, mro25\}@pitt.edu}}

\date{25 April 2017}

\begin{document}
\maketitle
\begin{abstract}
	We present the details of our system for stance detection of news headline--body pairs as a tool to assist manual fact checking or as a basis for an automated truth-labeling system. Training from a human-labeled sample of 40,000 headline--body pairs, our system calculates feature vectors for each pair and then trains an SVM classifier on these vector--stance pairs. Using this trained classifier we achieve 88.8\% accuracy on the development set and 87.8\% accuracy on the test set in the task of assigning a stance \textit{unrelated}, \textit{discusses}, \textit{agrees}, or \textit{disagrees} to each headline--body pair. We close by proposing directions for future work, including ways to improve the system's ability to distinguish among bodies which \textit{agree}, \textit{disagree}, and \textit{discuss}.
\end{abstract}

\section{Introduction}
The task of detecting low-quality or misleading news content in a fully or partly automated way has gained significant attention recently, as the limits of manual fact-checking become increasingly apparent with the ever-larger deluge of information of dubious quality released daily across the Web. Our work follows the partial automation approach to this quality assessment task, specifically addressing the task of labeling the relationship between a news item's body text to its headline as \textit{unrelated}, \textit{discusses}, \textit{agrees}, or \textit{disagrees}. Using a tool such as ours, a manual editor could quickly dismiss articles with misleading headlines and spend care on fact-checking in those articles which make coherent, falsifiable claims.

Our efforts follow the precedent set by~\cite{ferreira2016emergent}, who showed the viability of classifying known-related headline--body pairs, labeling each body as \textit{for}, \textit{against}, or \textit{observing} the claim made by the corresponding headline. In a similar task,~\cite{Augenstein16} automatically labeled tweets as holding a \textit{positive}, \textit{negative}, or \textit{neutral} stance toward an unspecified target, such that their system had first to identify the target and then the stance.

<<<<<<< Updated upstream
Last, we drew inspiration from the related task of question answering, treating the headline as a sort of query and the body text as a potentially related document. To this end, we attempted to leverage features developed in~\cite{Hasan2013},~\cite{Shen2007}, and~\cite{Surdeanu2011}. Each of these works uses semantic features to ascertain the relationship of the document body to the query---an insight we attempted to capture in our work.
=======
Last, we drew inspiration from the related task of question answering, treating the headline as a sort of query and the body text as a potentially related document. To this end, we leveraged features developed in~\cite{Meyer2003},~\cite{Shen2007}, and~\cite{Surdeanu2011}. Each of these works uses semantic features to ascertain the relationship of the document body to the query---an insight we attempted to capture in our work.
>>>>>>> Stashed changes

\section{Approach}
Our system makes use of two Python libraries: NLTK and scikit-learn. NLTK is a general natural language library which offers a variety of general functions such as tokenization (for both sentences and words) and part-of-speech tagging, as well as access to corpora and other lexical resources such as WordNet.~\cite{Bird2004}

The scikit-learn library (\texttt{sklearn}) contains tools for data analysis. In particular, our system leverages scikit-learn's implementation of a support vector machine (SVM) to build a classifier using the features extracted from each headline-body pair.~\cite{scikit-learn} Sklearn was chosen based off the results of ~\cite{ferreira2016emergent} who's system used sklearn's LogisticRegression classifier. The LogisticRegression from sklearn is a linear classifier, despite its name, so the SVM classifier was initially chosen for its ability to handle non-linear combinations of features. Despite this initial choice, it was found that a linear kernel performed almost as well as a non-linear kernel and achieved much better runtime performance.
\subsection{System Design}
The system performs training in two steps: feature extraction and classifier fitting. This is because the SVM classifier implemented by sklearn trains off of a full set of feature vectors and corresponding labels, so it requires the feature vectors and label for each training example before it fit the classifier. The classifier used was kept on default settings, with the exception of switching the kernel mode to linear from radial basis function, which was the default. The switch to linear was observed to have negligible affect on performance, while resulting in a more stable run time.

\subsection{Feature Selection}
Features were chosen based on fulfilling one of two criteria: related/unrelated disambiguation and agree/disagree/discusses disambiguation. Some features were chosen from features used by related systems, others were chosen by observed patterns in the test set. Each feature tested was implemented alongside the current established base feature set and the impact on accuracy and performance examined to determine whether the feature would be used. If a feature performed well, it was incorporated into the base feature set.
\section{Features}
The following are the features present in the final model:
\begin{itemize}
  \item \textbf{Bag of Words Comparison}\\For this feature, the headline and article body are both converted into bags of words, and each set has all stop words removed (stop words as defined by the NLTK stop word set~\cite{Bird2004}). The feature is the number of words in the headline word set which occur in the body. This was tested both normalized to the number of words in the headline set (putting the feature value between 0 and 1) and as a raw count, and there appeared to be no major difference between the two. This feature was chosen for it's success rate in other systems (including ~\cite{ferreira2016emergent}), and as expected it significantly increased the system's accuracy in distinguishing between unrelated and related headline/body pairs, though it did not affect intra-related disambiguation.
 \item \textbf{Cosine Similarity}\\For this feature, the headline and article body were both stripped of stop words and punctuation. The system then loops through each sentence in the body and compares to the headline sentence provided. The comparison is done by vectorizing each sentence and then performing cosine similarity between the sentences. The first feature returned is the average cosine similarity of each sentence in the body compared to the headline. The second feature returned is the max cosine similarity out of each sentence in the body compared to the headline. The third feature returned is once again getting the max similarity from each body compared to the headline, but the similarity measurement is a Sequence-Matcher provided by difflib that checks for the total number of sequence matches between the headline and the sentence from the body provided. 
\end{itemize}
\section{Results}
\subsection{Specifications}
For training, a corpora with 40,000 lines with the pattern of entry, body id, and stance was used. Also for training, all the different stances (agrees, disagrees, discusses, unrelated) are used. The test.csv testing data, testing gold-set of answers, and the python accuracy tool were provided by Yuhuan Jiang. The system used to evaluate the run-time was a 2.9 GHz Intel Core i5 processor. Table 1 references the results by Accuracy, Test Score, and Time taken to complete. 
\subsection{Results}
To evaluate the the accuracy and score, the use of several different combinations of features was needed to evaluate if the improved accuracy was worth the added time. The first feature added was the Bag-of-Words feature, which set the standard accuracy to .815 and a test score of 1761.0 with a run-time to model the training data at about 4 minutes. The uniqueness of headlines meant that features that may not seem like they would increase accuracy actually  may, so this lead to adding the second feature which was the length of the headline. This added no run-time but increased the accuracy to .835 with a test score of 1907.25.

The second set of features (Table 1: +SC) that were added were the max cosine similarity of headline to body-sentence, the average cosine similarity of headline to body, and the max headline to body-sentence ratio using a python library sequence-matcher. The run-time for this set of features took 14 to 15 minutes, and with the run-time added to the previous set of features it took about 18 minutes. The accuracy with these features applied to the training data was     .870 with a test score of 2100.25. The additional run-time was worth the .035 increase in accuracy and the 193 point test score increase. 

The third set of features that were added are the NSB features. There were many different NSB features, but the features selected [NATHANIEL ENTER***] were the features that improved the model. The run-time for this set of features took 5 to 6 minutes and with the previous run-times added on took about 20 to 21 minutes. The accuracy with these features applied to the training data increased the accuracy to .878 and increased the test score to 2149.25. 


\begin{table}
  \centering
  \caption{Results}

    \begin{tabular}{| l | l | l | l | l |}
    
    \hline
     & BOW & +HL & +SC & +NSB\\ \hline
    Accuracy & .815 & .835 & .870 & .878\\ \hline
    Test Score & 1761.0& 1907.25 & 2100.25 & 2149.25\\ \hline
    Time & $\sim$4 Min. &$\sim$4 Min. & $\sim$15 Min. & $\sim$21 Min. \\
    \hline
    \end{tabular}
    
\end{table}

<<<<<<< Updated upstream
We see from the confusion matrix (Table 2) that the final system is highly accurate at distinguishing between related and unrelated headline--body pairs. However, it fails in nearly every case to properly label related pairs which either agree or disagree, instead labeling nearly all related articles with \textit{discuss}. We propose that this result is due to our feature sets' inability to capture the deeper semantic properties of headlines and bodies; by considering only surface and lexical features, our system fails to align claims in the headline with related claims in the body. Given our system's high accuracy at labeling \textit{unrelated }headline--body pairs, we maintain that the system would be basically useful to editors for its ability to quickly label completely irrelevant news items from a list of articles to be curated, especially if the real-world corpus reflects the training and test data in its very high proportion of \textit{unrelated} pairs.

As future work, we thus propose the following possible routes for improving the system's performance, especially in distinguishing among \textit{agree}, \textit{disagree}, and \textit{discuss}. First, it is possible that using a single phase of classifier training prevented the SVM from finding useful patterns in the existing feature sets; to this end, we propose training one classifier to label a headline--body pair as either \textit{unrelated} or \textit{related} and then training a second classifier to label related pairs with \textit{agree}, \textit{disagree}, or \textit{discuss}. Moreover, we propose the addition of semantic-level features to those used during training and evaluation. During our project, we did implement a subject-frame-object (SFO) similarity metric inspired by that in~\cite{Hasan2013}, but because our topic domain is open unlike this paper's (which was restricted to four specific debate topics), we were unable to get high enough accuracy with semantic role labeling (using MaltParser and Semafor) to merit inclusion of this feature. Last, we suggest that this task could benefit from the \textbf{RootDist} feature described in~\cite{ferreira2016emergent}, which seems likely to capture the relationship of a body to a claim in a useful manner based on these authors' results.
=======
>>>>>>> Stashed changes

\section{Conclusion}
We present a model for detecting headline and body validity by using an SVM classifier. The classifier has specifically chosen features that only improve the accuracy and test score. Our system model is able to achieve a high accuracy in disambiguating between all the stances while being exceptionally well trained in disambiguating the relevant/irrelevant stances. We believe that this approach could be applied with more sophisticated features that could improve the ability for our classifier to disambiguate between agree, disagree, and discuss which would improve our overall system. For future work, we could experiment with a system to increase speed when adding the NSB and cosine similarity feature, as well as looking into a better disambiguation system which could improve our test score.  

\section*{Acknowledgments}

We probably don't have any.

\bibliography{paper}
\bibliographystyle{aclbib}

\appendix

\section{Supplemental Material}
If we need an appendix\ldots

\end{document}
